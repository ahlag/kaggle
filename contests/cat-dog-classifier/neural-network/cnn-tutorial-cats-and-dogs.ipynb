{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is part 2 for keras series click here for [part1](https://www.kaggle.com/bavalpreet26/keras-nb1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://keras.io/img/logo.png)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold\nimport itertools\nimport os\nimport shutil\nimport random\nimport glob\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_path = '/kaggle/input/cat-and-dog/training_set/training_set'\n# valid_path = 'data/dogs-vs-cats/valid'\ntest_path = '../input/cat-and-dog/test_set/test_set'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n    .flow_from_directory(directory=train_path, target_size=(224,224), classes=['cats', 'dogs'], batch_size=10)\ntest_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n    .flow_from_directory(directory=test_path, target_size=(224,224), classes=['cats', 'dogs'], batch_size=10, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. <font color='red'>ImageDataGenerator.flow_from_directory()</font> creates a DirectoryIterator, which generates batches of normalized tensor image data from the respective data directories.\n2. Notice, to ImageDataGenerator for each of the data sets, we specify <font color='red'>preprocessing_function=tf.keras.applications.vgg16.preprocess_input</font>. For now, just understand this does an additional processing step on the images. We'll cover what exactly this processing is when we work with the pre-trained VGG16 CNN in a future episode.\n3. To <font color = 'red'>flow_from_directory()</font>, we first specify the path for the data. We then specify the target_size of the images, which will resize all images to the specified size. The size we specify here is determined by the input size that the neural network expects.\n4. The <font color = 'red'>classes</font> parameter expects a list that contains the underlying class names, and lastly, we specify the batch_size.\n5. We also specify <font color = 'red'>shuffle=False</font> only for <font color = 'red'>test_batches</font>. That's because, later when we plot the evaluation results from the model to a <font color = 'red'>confusion matrix</font>,we'll need to able to access the unshuffled labels for the test set. By default, the data sets are shuffled\n6. <font color = 'blue'>Note, in the case where you do not know the labels for the test data, you will need to modify the `test_batches` variable. Specifically, the change will be to set the parameters `classes = None` and `class_mode = None` in `flow_from_directory()`.</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Visualize the data\n\nWe now call `next(train_batches)` to generate a batch of images and labels from the training set. Note that the size of this batch is determined by the `batch_size` we set when we created `train_batches`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### <font color = 'green'>Preparing the Train Data</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs, labels = next(train_batches)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then use this plotting function obtained from `TensorFlow's documentation` to plot the processed images within our Jupyter notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotImages(images_arr):\n    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotImages(imgs)\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is what the first processed random batch from the training set looks like. Notice that the color appears to be distorted. This has to do with the VGG16 processing we applied to the data sets, which we'll talk about in few minutes. Don't worry about it for now, just know that the RGB pixel data has been processed in such a way that the image data now looks like this before being passed to the network.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Note that dogs are represented with the `one-hot encoding` of `[0,1]`, and cats are represented by `[1,0]`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We’re now all set up to work with this data!, we’ll now use this data to train a convolutional neural network.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Build A Simple CNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To build the CNN, we’ll use a Keras Sequential model. Recall, we first introduced a Sequential model in an earlier [tutorial](https://www.kaggle.com/bavalpreet26/keras-nb1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)),\n    MaxPool2D(pool_size=(2, 2), strides=2),\n    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n    MaxPool2D(pool_size=(2, 2), strides=2),\n    Flatten(),\n    Dense(units=2, activation='softmax')\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first layer in the model is a 2-dimensional convolutional layer. This layer will have `32` output filters each with a kernel size of `3x3`, and we’ll use the `relu` activation function.\n\nNote that the choice for the number of output filters specified is arbitrary, and the chosen kernel size of `3x3` is generally a very common size to use. You can experiment by choosing different values for these parameters.\n\nWe enable zero-padding by specifying `padding = 'same'`.\n\nOn the first layer only, we also specify the `input_shape`, which is the shape of our data. Our images are `224` pixels high and `224` pixels wide and have `3` color channels: RGB. This gives us an `input_shape` of `(224,224,3)`.\n\nWe then add a `max pooling` layer to pool and reduce the dimensionality of the data.\n\nWe follow this by adding another convolutional layer with the exact specs as the earlier one, except for this second `Conv2D` layer has `64` filters. The choice of 64 here is again arbitrary, but the general choice of having more filters in later layers than in earlier ones is common. This layer is again followed by the same type of `MaxPool2D` layer\n\nWe then `Flatten` the output from the convolutional layer and pass it to a `Dense` layer. This `Dense` layer is the output layer of the network, and so it has `2` nodes, one for cat and one for dog. We’ll use the `softmax` activation function on our output so that the output for each sample is a probability distribution over the outputs of cat and dog.\n\nWe can check out a summary of the model by calling `model.summary()`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the model is built, we `compile` the model using the `Adam optimizer` with a learning rate of `0.0001`, a loss of `categorical_cross_entropy`, and we’ll look at `accuracy` as our performance metric. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that when we have only two classes, we could instead configure our output layer to have only one output, rather than two, and use `binary_crossentropy` as our loss, rather than `categorical_crossentropy`. Both options work equally well and achieve the exact same result.\n\nWith `binary_crossentropy`, however, the last layer would need to use <font color='orange'>sigmoid</font>, rather than <font color='dark pink'>softmax</font>, as its activation function.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Train A Simple CNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now it’s time to train the model.\n\nWe've already introduced the `model.fit()` function to train a model in above cells. We'll be using it in the same fashion here, except for now, we'll be passing in our newly introduced `DirectoryIterators` `train_batches`to train the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=train_batches, epochs=5, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We’re specifying `5` as the number of `epochs` we’d like to run, and setting the `verbose` parameter to `2`, which just specifies the verbosity of the log output printed to the console during training.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From this output, we can see the performance of this simple model on the training set is great, with accuracy reaching almost 100% and loss nearing almost 0,we can see that our model is vastly overfitting the training data.\n\nAt this point, we could continue to work on this model to combat overfitting, or we could try another approach of using a pre-trained model on this data. We'll explore the latter in the upcoming cells.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Make Predictions With A Keras CNN Image Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### <font color = 'green'>Preparing the Test Data</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We’ll now run `next(test_batches)` to extract a batch of images and their corresponding labels from the test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_imgs, test_labels = next(test_batches)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the `plotImages()` function we previously introduced, we can see what this batch of test data looks like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plotImages(test_imgs)\nprint(test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just as we saw before, cats are labeled with a one-hot encoding of `[1,0]`, and dogs are labeled as `[0,1]`.\n\nNote, because we chose to not shuffle our test set when we originally created it, the first half of the test data is all cats, and the second half is all dogs. Also, recall that the color data appears skewed due to the VGG16 preprocessing we specified when we created the data sets.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Predicting On The Test Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we’ll use our previously built model and call model.predict() to have the model predict on the test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(x=test_batches, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We pass in the test set, `test_batches`, and set `verbose=0` to see no output during the evaluation process.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After running the predictions, we can print our the rounded predictions see what they look like.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.round(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the labels that the model is predicting for our images.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Plotting Predictions With A Confusion Matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To get a better visualization of these results, we’ll plot them in a confusion matrix.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We create the confusion matrix using `scikit-learn`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To the confusion matrix, we pass the true labels of the test set, along with the predicted labels for the test set from the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Note, we can access the unshuffled true labels for the test set by calling `test_batches.classes`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We transform the one-hot encoded predicted labels to be in the same format as the true labels by only selecting the element with the highest value for each prediction using `np.argmax(predictions, axis=-1)`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now use the plot_confusion_matrix() function that is copied directly from scikit-learn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then inspect the `class_indices` for the labels so that we know in which order to pass them to our confusion matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_batches.class_indices","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Finally, we plot the confusion matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_plot_labels = ['cat','dog']\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the model correctly predicted that an image was a cat `620` times when it actually was a cat, and it incorrectly predicted that an image was a cat `391` times when it was not a cat. It correctly predicted that an image was a dog `768` times, and incorrectly predicted that an image was a dog `244` times.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# optional - how to do cross validation?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"no_classes = 100\nno_epochs = 5\n# optimizer = Adam()\nverbosity = 1\nnum_folds = 5\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=num_folds, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\nfor train, test in kfold.split(imgs, labels):\n\n  # Define the model architecture\n    model = Sequential([\n    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)),\n    MaxPool2D(pool_size=(2, 2), strides=2),\n    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n    MaxPool2D(pool_size=(2, 2), strides=2),\n    Flatten(),\n    Dense(units=2, activation='softmax')])\n\n  # Compile the model\n    model.compile(optimizer=Adam(learning_rate=0.0001),\n                  loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n  # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n\n  # Fit data to model\n    history = model.fit(x=train_batches, epochs=5, verbose=2)\n\n  # Generate generalization metrics\n    scores = model.evaluate(imgs[test], labels[test], verbose=0)\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])\n\n  # Increase fold number\n    fold_no = fold_no + 1\n\n# == Provide average scores ==\nprint('------------------------------------------------------------------------')\nprint('Score per fold')\nfor i in range(0, len(acc_per_fold)):\n    print('------------------------------------------------------------------------')\n    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\nprint('------------------------------------------------------------------------')\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\nprint(f'> Loss: {np.mean(loss_per_fold)}')\nprint('------------------------------------------------------------------------')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"these all tutorials are inspired from this [channel](https://www.youtube.com/watch?v=LhEMXbjGV_4&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL&index=11)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that we have a general understanding for how to build and work with a CNN using Keras, we'll now move on to working with a pre-trained model on this data set, which we will see will generalize much better!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}